{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MusicGen-RepEng\n",
    "Welcome to MusicGen-RepEng's demo jupyter notebook. Here you will find a series of self-contained examples of how to use MusicGen with Representation Engineering.\n",
    "\n",
    "First, we start by initializing MusicGen, you can choose a model from the following selection:\n",
    "1. `facebook/musicgen-small` - 300M transformer decoder.\n",
    "2. `facebook/musicgen-medium` - 1.5B transformer decoder.\n",
    "3. `facebook/musicgen-melody` - 1.5B transformer decoder also supporting melody conditioning.\n",
    "4. `facebook/musicgen-large` - 3.3B transformer decoder.\n",
    "\n",
    "We will use the `facebook/musicgen-small` variant for the purpose of this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.models import MultiBandDiffusion\n",
    "\n",
    "USE_DIFFUSION_DECODER = False\n",
    "# Using small model, better results would be obtained with `medium` or `large`.\n",
    "model = MusicGen.get_pretrained('facebook/musicgen-medium', device=\"cuda\")\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    mbd = MultiBandDiffusion.get_mbd_musicgen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us configure the generation parameters. Specifically, you can control the following:\n",
    "* `use_sampling` (bool, optional): use sampling if True, else do argmax decoding. Defaults to True.\n",
    "* `top_k` (int, optional): top_k used for sampling. Defaults to 250.\n",
    "* `top_p` (float, optional): top_p used for sampling, when set to 0 top_k is used. Defaults to 0.0.\n",
    "* `temperature` (float, optional): softmax temperature parameter. Defaults to 1.0.\n",
    "* `duration` (float, optional): duration of the generated waveform. Defaults to 30.0.\n",
    "* `cfg_coef` (float, optional): coefficient used for classifier free guidance. Defaults to 3.0.\n",
    "\n",
    "When left unchanged, MusicGen will revert to its default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=250,\n",
    "    duration=0.02\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can go ahead and start generating music using one of the following modes:\n",
    "* Unconditional samples using `model.generate_unconditional`\n",
    "* Music continuation using `model.generate_continuation`\n",
    "* Text-conditional samples using `model.generate`\n",
    "* Melody-conditional samples using `model.generate_with_chroma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "music, sr = torchaudio.load('/home/sake/MusicGenRepEng_Dataset/Rock/Alternative Rock/Nirvana - Smells Like Teen Spirit.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(music[:,:50].repeat(2,1,1) == torch.stack([music[:,:50], music[:,:50]], dim=0)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=250,\n",
    "    duration=8,\n",
    "    two_step_cfg=False\n",
    ")\n",
    "\n",
    "# Here we use a synthetic signal to prompt both the tonality and the BPM\n",
    "# of the generated audio.\n",
    "res = model.generate_continuation(\n",
    "    music[:,:int(sr*0.02)].repeat(2,1,1),\n",
    "    sr, ['rock, energetic', \n",
    "            'rock, sleepy'], \n",
    "    progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Text Condition Representations(Hidden States)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "music, sr = torchaudio.load('/home/sake/MusicGenRepEng_Dataset/Rock/Alternative Rock/Nirvana - Smells Like Teen Spirit.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=250,\n",
    "    duration=0.02,\n",
    "    two_step_cfg=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = model.get_hidden_states(music[:,:int(sr*0.02)].repeat(2,1,1), sr, [\"fast tempo\", \"slow tempo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes, _ = model._prepare_tokens_and_attributes([\"techno, fast beats, happy, hard, joyful, tribal\"], None)\n",
    "attributes, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.lm.condition_provider(model.lm.condition_provider.tokenize(attributes))\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings['description'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = model.get_hidden_states_text_condition([\"techno, fast beats\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm.condition_provider.conditioners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm.condition_provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm.condition_provider(model.lm.condition_provider.tokenize(attributes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm.condition_provider.conditioners.description(attributes[0]['text']['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm.condition_provider(attributes[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Representations(Hidden States)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "music, sr = torchaudio.load('/home/sake/MusicGenRepEng_Dataset/Rock/Alternative Rock/Nirvana - Smells Like Teen Spirit.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_music = music[:, 20*i*sr:20*(i+1)*sr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = model.get_hidden_states(\n",
    "    input_music, \n",
    "    sr, None, \n",
    "    progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = []\n",
    "for i in range(10):\n",
    "    input_music = music[:, 20*i*sr:20*(i+1)*sr]\n",
    "    rep = model.get_hidden_states(\n",
    "        input_music,\n",
    "        sr, None,\n",
    "        progress=True)\n",
    "    reps.append(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_vec = torch.stack(rep, dim=1)[:,500:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(reps, \"/home/sake/Nirvana - Smells Like Teen Spirit_MusicGenRepEng_Dataset_hidden_states_every20s_10t.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm \n",
    "\n",
    "for path in tqdm(Path('/home/sake/MusicGenRepEng_Dataset_separated').rglob('*.mp3')):\n",
    "    print(\"Representing: \", path)\n",
    "    out_path = str(path).replace('MusicGenRepEng_Dataset', 'MusicGenRepEng_Dataset_hidden_states_30-60_mid10')\n",
    "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    music, sr = torchaudio.load(str(path))\n",
    "    input_music = music[:, 30*sr:50*sr]\n",
    "    rep = model.read_representations(\n",
    "        input_music, \n",
    "        sr, None, \n",
    "        progress=True)\n",
    "    rep_vec = torch.stack(rep, dim=1)[:,500:1000].mean(1)\n",
    "    torch.save(rep_vec, path.with_suffix('.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Control Vector - Text Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_onto_direction(H, direction):\n",
    "    \"\"\"Project matrix H (n, d_1) onto direction vector (d_2,)\"\"\"\n",
    "    mag = np.linalg.norm(direction)\n",
    "    assert not np.isinf(mag)\n",
    "    return (H @ direction) / mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_pairs = []\n",
    "for path in tqdm(Path('/home/sake/MusicGenRepEng_Dataset_50ms_energetic_sleepy_mediummodel_rock_norm_nob4layer').rglob('*.pt')):\n",
    "    loaded = torch.load(str(path))[-1]\n",
    "    representation_pairs.append(loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_pairs = torch.cat(representation_pairs, dim=0)\n",
    "representation_pairs = representation_pairs.permute(1,0,2)\n",
    "representation_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_layer_hiddens = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer, pair in enumerate(representation_pairs):\n",
    "        relative_layer_hiddens[layer] = (\n",
    "            pair[::2] - pair[1::2]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_layer_hiddens[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(48):\n",
    "    print(i, (relative_layer_hiddens[i][0]==0).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = {}\n",
    "for layer in range(len(relative_layer_hiddens)):\n",
    "    # assert representation_pairs[layer].shape[0] == 110 * 2\n",
    "\n",
    "    # fit layer directions\n",
    "    train = np.vstack(\n",
    "        relative_layer_hiddens[layer].to(\"cpu\").numpy()\n",
    "        - relative_layer_hiddens[layer].to(\"cpu\").numpy().mean(axis=0, keepdims=True)\n",
    "    )\n",
    "    pca_model = PCA(n_components=1, whiten=False).fit(train)\n",
    "    # shape (n_features,)\n",
    "    directions[layer] = pca_model.components_.astype(np.float32).squeeze(axis=0)\n",
    "\n",
    "    # calculate sign\n",
    "    projected_hiddens = project_onto_direction(\n",
    "        representation_pairs[layer].to(\"cpu\").numpy(), directions[layer]\n",
    "    )\n",
    "\n",
    "    # order is [positive, negative, positive, negative, ...]\n",
    "    positive_smaller_mean = np.mean(\n",
    "        [\n",
    "            projected_hiddens[i] < projected_hiddens[i + 1]\n",
    "            for i in range(0, representation_pairs.shape[1], 2)\n",
    "        ]\n",
    "    )\n",
    "    positive_larger_mean = np.mean(\n",
    "        [\n",
    "            projected_hiddens[i] > projected_hiddens[i + 1]\n",
    "            for i in range(0, representation_pairs.shape[1], 2)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if positive_smaller_mean > positive_larger_mean:  # type: ignore\n",
    "        directions[layer] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions[47].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Control Vector - A song pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ditto_reps = torch.load(\"/home/sake/Ditto-2-NewJeans_MusicGenRepEng_Dataset_hidden_states_every20s_10t.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dexter_reps = torch.load(\"/home/sake/Ricardo Villalobos - Dexter [SED008]_MusicGenRepEng_Dataset_hidden_states_every20s_10t.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ditto_reps[0][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.stack([rep[-1] for rep in ditto_reps], dim=0)\n",
    "target = target.squeeze(1).permute(1, 0, 2).cpu()\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = torch.stack([rep[-1] for rep in dexter_reps], dim=0)\n",
    "reps = reps.squeeze(1).permute(1, 0, 2).cpu()\n",
    "reps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = torch.stack([rep[-1] for rep in reps], dim=0)\n",
    "reps = reps.squeeze(1).permute(1, 0, 2).cpu()\n",
    "reps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_onto_direction(H, direction):\n",
    "    \"\"\"Project matrix H (n, d_1) onto direction vector (d_2,)\"\"\"\n",
    "    mag = np.linalg.norm(direction)\n",
    "    assert not np.isinf(mag)\n",
    "    return (H @ direction) / mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Difference\n",
    "\n",
    "diffs = target.cpu() - reps.cpu() # target - others (pos - neg)\n",
    "diffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg or Last Hidden State\n",
    "\n",
    "directions = {}\n",
    "for layer in tqdm(range(diffs.shape[0])):\n",
    "    # assert diff[layer].shape[0] == len(inputs) * 2\n",
    "\n",
    "    # fit layer directions\n",
    "    train = np.vstack(\n",
    "        diffs[layer]\n",
    "        # - diffs[layer].mean(axis=0, keepdims=True)\n",
    "    )\n",
    "    pca_model = PCA(n_components=1, whiten=False).fit(train)\n",
    "    # shape (n_features,)\n",
    "    directions[layer] = pca_model.components_.astype(np.float32).squeeze(axis=0)\n",
    "    # print(directions[layer].shape)\n",
    "    # calculate sign\n",
    "    # projected_hiddens = project_onto_direction(\n",
    "    #     reps[layer], directions[layer]\n",
    "    # )\n",
    "    # # print(projected_hiddens[0])\n",
    "    # target_projected_hiddens = project_onto_direction(\n",
    "    #     target[layer], directions[layer]\n",
    "    # )\n",
    "    # # print(target_projected_hiddens[0])\n",
    "\n",
    "    # # order is [positive, negative, positive, negative, ...]\n",
    "    # positive_smaller_mean = np.mean(\n",
    "    #     [\n",
    "    #         target_projected_hiddens[0] < projected_hiddens[i] # target is smaller than others\n",
    "    #         for i in range(0, reps.shape[1])\n",
    "    #     ]\n",
    "    # )\n",
    "    # positive_larger_mean = np.mean(\n",
    "    #     [\n",
    "    #         target_projected_hiddens[0] > projected_hiddens[i] # target is larger than others\n",
    "    #         for i in range(0, reps.shape[1])\n",
    "    #     ]\n",
    "    # )\n",
    "\n",
    "    # if positive_smaller_mean > positive_larger_mean:  # type: ignore\n",
    "    #     directions[layer] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ditto_smells_directions = directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Control Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = []\n",
    "for path in tqdm(Path('/home/sake/MusicGenRepEng_Dataset_hidden_states_30-60_mid10_non_avg_smallmodel').rglob('*.pt')):\n",
    "    rep = torch.load(path)\n",
    "    reps.append(rep.cpu())\n",
    "reps = torch.stack(reps)\n",
    "reps = reps.squeeze(1)\n",
    "# reps = reps.permute(1, 0, 2) # (layers, batch, hidden_states)\n",
    "reps = reps.permute(2, 0, 1, 3) # (layers, batch, timesteps, hidden_states)\n",
    "# reps = reps[:,:,-1] # Last hidden_state\n",
    "reps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.load(\"/home/sake/MusicGenRepEng_Dataset_hidden_states_30-60_mid10/Ditto-2-NewJeans.pt\")\n",
    "target = target.permute(1, 0, 2).cpu() # (layers, batch, hidden_states)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = rep_vec.cpu()\n",
    "target = target.permute(2, 0, 1, 3) # (layers, batch, timesteps, hidden_states)\n",
    "# target = target[:,:,-1] # Last hidden_state\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Difference\n",
    "\n",
    "diffs = target.cpu() - reps.cpu() # target - others (pos - neg)\n",
    "diffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_onto_direction(H, direction):\n",
    "    \"\"\"Project matrix H (n, d_1) onto direction vector (d_2,)\"\"\"\n",
    "    mag = np.linalg.norm(direction)\n",
    "    assert not np.isinf(mag)\n",
    "    return (H @ direction) / mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg or Last Hidden State\n",
    "\n",
    "directions = {}\n",
    "for layer in tqdm(range(diffs.shape[0])):\n",
    "    # assert diff[layer].shape[0] == len(inputs) * 2\n",
    "\n",
    "    # fit layer directions\n",
    "    train = np.vstack(\n",
    "        diffs[layer]\n",
    "        - diffs[layer].mean(axis=0, keepdims=True)\n",
    "    )\n",
    "    pca_model = PCA(n_components=1, whiten=False).fit(train)\n",
    "    # shape (n_features,)\n",
    "    directions[layer] = pca_model.components_.astype(np.float32).squeeze(axis=0)\n",
    "    # print(directions[layer].shape)\n",
    "    # calculate sign\n",
    "    projected_hiddens = project_onto_direction(\n",
    "        reps[layer], directions[layer]\n",
    "    )\n",
    "    # print(projected_hiddens[0])\n",
    "    target_projected_hiddens = project_onto_direction(\n",
    "        target[layer], directions[layer]\n",
    "    )\n",
    "    # print(target_projected_hiddens[0])\n",
    "\n",
    "    # order is [positive, negative, positive, negative, ...]\n",
    "    positive_smaller_mean = np.mean(\n",
    "        [\n",
    "            target_projected_hiddens[0] < projected_hiddens[i] # target is smaller than others\n",
    "            for i in range(0, reps.shape[1])\n",
    "        ]\n",
    "    )\n",
    "    positive_larger_mean = np.mean(\n",
    "        [\n",
    "            target_projected_hiddens[0] > projected_hiddens[i] # target is larger than others\n",
    "            for i in range(0, reps.shape[1])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if positive_smaller_mean > positive_larger_mean:  # type: ignore\n",
    "        directions[layer] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_diffs = diffs.flatten(1,2).cpu()\n",
    "f_target = target.flatten(1,2).cpu()\n",
    "f_reps = reps.flatten(1,2).cpu()\n",
    "f_diffs.shape, f_target.shape, f_reps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Hidden States\n",
    "\n",
    "\n",
    "directions = {}\n",
    "for layer in tqdm(range(f_diffs.shape[0])):\n",
    "    # assert diff[layer].shape[0] == len(inputs) * 2\n",
    "\n",
    "    # fit layer directions\n",
    "    train = np.vstack(\n",
    "        f_diffs[layer]\n",
    "        - f_diffs[layer].mean(axis=0, keepdims=True)\n",
    "    )\n",
    "    pca_model = PCA(n_components=1, whiten=False).fit(train)\n",
    "    # shape (n_features,)\n",
    "    directions[layer] = pca_model.components_.astype(np.float32).squeeze(axis=0)\n",
    "    # calculate sign\n",
    "    projected_hiddens = project_onto_direction(\n",
    "        f_reps[layer], directions[layer]\n",
    "    )\n",
    "    target_projected_hiddens = project_onto_direction(\n",
    "        f_target[layer], directions[layer]\n",
    "    )\n",
    "\n",
    "    # order is [positive, negative, positive, negative, ...]\n",
    "    positive_smaller_mean = np.mean(\n",
    "        [\n",
    "            target_projected_hiddens[i%f_target.shape[1]] < projected_hiddens[i] # target is smaller than others\n",
    "            for i in range(0, f_reps.shape[1])\n",
    "        ]\n",
    "    )\n",
    "    positive_larger_mean = np.mean(\n",
    "        [\n",
    "            target_projected_hiddens[i%f_target.shape[1]] > projected_hiddens[i] # target is larger than others\n",
    "            for i in range(0, f_reps.shape[1])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if positive_smaller_mean > positive_larger_mean:  # type: ignore\n",
    "        directions[layer] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(directions, \"/home/sake/Ditto-2-NewJeans_MusicGenRepEng_Dataset_hidden_states_30-60_non_avg_smallmodel_directions.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with Control Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.utils.notebook import display_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "music, sr = torchaudio.load('/home/sake/MusicGenRepEng_Dataset/Rock/Alternative Rock/Nirvana - Smells Like Teen Spirit.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "# From https://gist.github.com/gatheluck/c57e2a40e3122028ceaecc3cb0d152ac\n",
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=250,\n",
    "    duration=10,\n",
    "    two_step_cfg=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"rock\"]*n, control_vectors=[directions], coefficients=[0.15], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"rock\"]*n, control_vectors=[directions], coefficients=[0.1], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"rock\"]*n, control_vectors=[directions], coefficients=[0.07], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"rock\"]*n, control_vectors=[directions], coefficients=[0.06], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"rock\"]*n, control_vectors=[directions], coefficients=[0.05], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"rock\"]*n, control_vectors=[directions], coefficients=[0.04], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"rock\"]*n, control_vectors=[directions], coefficients=[0.03], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"rock\"]*n, control_vectors=[directions], coefficients=[0.02], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"rock\"]*n, control_vectors=[directions], coefficients=[0.01], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"rock\"]*n, control_vectors=[directions], coefficients=[0.00], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"rock\"]*n, control_vectors=[directions], coefficients=[-0.01], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"rock\"]*n, control_vectors=[directions], coefficients=[-0.02], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"rock\"]*n, control_vectors=[directions], coefficients=[-0.03], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"rock\"]*n, control_vectors=[directions], coefficients=[-0.04], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"rock\"]*n, control_vectors=[directions], coefficients=[-0.05], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"rock\"]*n, control_vectors=[directions], coefficients=[-0.06], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"rock\"]*n, control_vectors=[directions], coefficients=[-0.07], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"rock\"]*n, control_vectors=[directions], coefficients=[-0.1], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"rock\"]*n, control_vectors=[directions], coefficients=[-0.15], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +0.15\n",
    "n = 4\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*0.02)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[directions], coefficients=[0.15], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"rock\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +0.15\n",
    "n = 4\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*0.02)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[directions, directions], coefficients=[0.15, 0.1], sustains=[100, 10], ramps=[250, 50],\n",
    "                                                       before_layer=False, descriptions=[\"rock\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torchaudio\n",
    "import torch\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "def get_bip_bip(bip_duration=0.125, frequency=440,\n",
    "                duration=0.5, sample_rate=32000, device=\"cuda\"):\n",
    "    \"\"\"Generates a series of bip bip at the given frequency.\"\"\"\n",
    "    t = torch.arange(\n",
    "        int(duration * sample_rate), device=\"cuda\", dtype=torch.float) / sample_rate\n",
    "    wav = torch.cos(2 * math.pi * 440 * t)[None]\n",
    "    tp = (t % (2 * bip_duration)) / (2 * bip_duration)\n",
    "    envelope = (tp >= 0.5).float()\n",
    "    return wav * envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use a synthetic signal to prompt both the tonality and the BPM\n",
    "# of the generated audio.\n",
    "res = model.generate_continuation(\n",
    "    get_bip_bip(0.125).expand(2, -1, -1), \n",
    "    32000, ['Jazz jazz and only jazz', \n",
    "            'Heartful EDM with beautiful synths and chords'], \n",
    "    progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use any audio from a file. Make sure to trim the file if it is too long!\n",
    "prompt_waveform, prompt_sr = torchaudio.load(\"../assets/bach.mp3\")\n",
    "prompt_duration = 2\n",
    "prompt_waveform = prompt_waveform[..., :int(prompt_duration * prompt_sr)]\n",
    "output = model.generate_continuation(prompt_waveform, prompt_sample_rate=prompt_sr, progress=True, return_tokens=True)\n",
    "display_audio(output[0], sample_rate=32000)\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    out_diffusion = mbd.tokens_to_wav(output[1])\n",
    "    display_audio(out_diffusion, sample_rate=32000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-conditional Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "output = model.generate(\n",
    "    descriptions=[\n",
    "        #'80s pop track with bassy drums and synth',\n",
    "        #'90s rock song with loud guitars and heavy drums',\n",
    "        #'Progressive rock drum and bass solo',\n",
    "        #'Punk Rock song with loud drum and power guitar',\n",
    "        #'Bluesy guitar instrumental with soulful licks and a driving rhythm section',\n",
    "        #'Jazz Funk song with slap bass and powerful saxophone',\n",
    "        'drum and bass beat with intense percussions'\n",
    "    ],\n",
    "    progress=True, return_tokens=True\n",
    ")\n",
    "display_audio(output[0], sample_rate=32000)\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    out_diffusion = mbd.tokens_to_wav(output[1])\n",
    "    display_audio(out_diffusion, sample_rate=32000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melody-conditional Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "model = MusicGen.get_pretrained('facebook/musicgen-melody')\n",
    "model.set_generation_params(duration=8)\n",
    "\n",
    "melody_waveform, sr = torchaudio.load(\"../assets/bach.mp3\")\n",
    "melody_waveform = melody_waveform.unsqueeze(0).repeat(2, 1, 1)\n",
    "output = model.generate_with_chroma(\n",
    "    descriptions=[\n",
    "        '80s pop track with bassy drums and synth',\n",
    "        '90s rock song with loud guitars and heavy drums',\n",
    "    ],\n",
    "    melody_wavs=melody_waveform,\n",
    "    melody_sample_rate=sr,\n",
    "    progress=True, return_tokens=True\n",
    ")\n",
    "display_audio(output[0], sample_rate=32000)\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    out_diffusion = mbd.tokens_to_wav(output[1])\n",
    "    display_audio(out_diffusion, sample_rate=32000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b02c911f9b3627d505ea4a19966a915ef21f28afb50dbf6b2115072d27c69103"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
