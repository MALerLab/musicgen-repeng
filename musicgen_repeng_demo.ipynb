{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MusicGen-RepEng\n",
    "Welcome to MusicGen-RepEng's demo jupyter notebook. Here you will find a series of self-contained examples of how to use MusicGen with Representation Engineering.\n",
    "\n",
    "First, we start by initializing MusicGen, you can choose a model from the following selection:\n",
    "1. `facebook/musicgen-small` - 300M transformer decoder.\n",
    "2. `facebook/musicgen-medium` - 1.5B transformer decoder.\n",
    "3. `facebook/musicgen-melody` - 1.5B transformer decoder also supporting melody conditioning.\n",
    "4. `facebook/musicgen-large` - 3.3B transformer decoder.\n",
    "\n",
    "We will use the `facebook/musicgen-small` variant for the purpose of this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.models import MultiBandDiffusion\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "USE_DIFFUSION_DECODER = False\n",
    "# Using small model, better results would be obtained with `medium` or `large`.\n",
    "model = MusicGen.get_pretrained('facebook/musicgen-large', device=\"cuda\")\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    mbd = MultiBandDiffusion.get_mbd_musicgen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us configure the generation parameters. Specifically, you can control the following:\n",
    "* `use_sampling` (bool, optional): use sampling if True, else do argmax decoding. Defaults to True.\n",
    "* `top_k` (int, optional): top_k used for sampling. Defaults to 250.\n",
    "* `top_p` (float, optional): top_p used for sampling, when set to 0 top_k is used. Defaults to 0.0.\n",
    "* `temperature` (float, optional): softmax temperature parameter. Defaults to 1.0.\n",
    "* `duration` (float, optional): duration of the generated waveform. Defaults to 30.0.\n",
    "* `cfg_coef` (float, optional): coefficient used for classifier free guidance. Defaults to 3.0.\n",
    "\n",
    "When left unchanged, MusicGen will revert to its default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=250,\n",
    "    duration=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can go ahead and start generating music using one of the following modes:\n",
    "* Unconditional samples using `model.generate_unconditional`\n",
    "* Music continuation using `model.generate_continuation`\n",
    "* Text-conditional samples using `model.generate`\n",
    "* Melody-conditional samples using `model.generate_with_chroma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 64\n",
    "res = model.generate(descriptions=[None]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "ditto, sr = torchaudio.load('/home/sake/ditto_vocalless.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 64\n",
    "res = model.generate_continuation(descriptions=[None]*n, prompt=ditto[...,30*sr:int(31.5*sr)].repeat(n,1,1), prompt_sample_rate=sr, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 128\n",
    "res = model.get_hidden_states_no_continuation(descriptions=[None]*n, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res), res[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "res = model.get_hidden_states_no_continuation(descriptions=[None]*n, progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Distance Mean/Var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_distance(target_path, opposite_path):\n",
    "    target_representations = []\n",
    "    for p in tqdm(natsorted(Path(target_path).rglob('*.pt'), key=str)):\n",
    "        loaded = torch.load(str(p), map_location=torch.device('cpu')).to(torch.float32)\n",
    "        target_representations.append(loaded)\n",
    "\n",
    "    target_representations = torch.cat(target_representations, dim=0)\n",
    "\n",
    "    opposite_representations = []\n",
    "    for p in tqdm(natsorted(Path(opposite_path).rglob('*.pt'), key=str)):\n",
    "        loaded = torch.load(str(p), map_location=torch.device('cpu')).to(torch.float32)\n",
    "        opposite_representations.append(loaded)\n",
    "\n",
    "    opposite_representations = torch.cat(opposite_representations, dim=0)\n",
    "\n",
    "    dist = (target_representations - opposite_representations)\n",
    "    dist = dist.permute(1,0,2)\n",
    "    dist = dist.pow(2).sum(-1).sqrt()\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = \"/home/sake/MusicGenRepEng_2.5s_ditto_hiddens_batch128_meanssss\"\n",
    "opposite_path = \"/home/sake/MusicGenRepEng_125tokens_uncond_hiddens_batch128_meanssss\"\n",
    "dist = get_distance(target_path, opposite_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dist.mean(-1), label='mean')\n",
    "plt.plot(dist.var(-1), label='var')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ditto_dist_mean = dist.mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ditto_dist_mean = ditto_dist_mean/ditto_dist_mean.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ditto_dist_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_pca(target_path, opposite_path):\n",
    "    target_representations = []\n",
    "    for p in tqdm(natsorted(Path(target_path).rglob('*.pt'), key=str)):\n",
    "        loaded = torch.load(str(p), map_location=torch.device('cpu'))\n",
    "        target_representations.append(loaded)\n",
    "\n",
    "    target_representations = torch.cat(target_representations, dim=0)\n",
    "\n",
    "    opposite_representations = []\n",
    "    for p in tqdm(natsorted(Path(opposite_path).rglob('*.pt'), key=str)):\n",
    "        loaded = torch.load(str(p), map_location=torch.device('cpu'))\n",
    "        opposite_representations.append(loaded)\n",
    "\n",
    "    opposite_representations = torch.cat(opposite_representations, dim=0)\n",
    "\n",
    "    representations = torch.cat([target_representations, opposite_representations], dim=0)\n",
    "    representations = representations.permute(1,0,2)\n",
    "\n",
    "    for layer, representation in enumerate(representations):\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(representation)\n",
    "        representation_scaled = scaler.transform(representation)\n",
    "        pca = PCA(n_components=2).fit(representation_scaled)\n",
    "        representation_pca = pca.transform(representation_scaled)\n",
    "        \n",
    "        target_representation_pca = representation_pca[:len(target_representations)]\n",
    "        opposite_representation_pca = representation_pca[len(target_representations):]\n",
    "\n",
    "        plt.scatter(target_representation_pca[:,0], target_representation_pca[:,1], color = 'red', alpha = 0.3, s=1, label = 'ditto')\n",
    "        plt.scatter(opposite_representation_pca[:,0], opposite_representation_pca[:,1], color = 'green', alpha = 0.3, s=1, label = 'oppposite')\n",
    "\n",
    "        plt.xlabel('component 0')\n",
    "        plt.ylabel('component 1')\n",
    "        plt.title(f\"Layer {layer}\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "def plot_pca_over_opposite(target_path, opposite_path):\n",
    "    target_representations = []\n",
    "    for p in tqdm(Path(target_path).rglob('*.pt')):\n",
    "        loaded = torch.load(str(p), map_location=torch.device('cpu'))\n",
    "        target_representations.append(loaded)\n",
    "\n",
    "    target_representations = torch.cat(target_representations, dim=0)\n",
    "    target_representations = target_representations.permute(1,0,2)\n",
    "\n",
    "    opposite_representations = []\n",
    "    for p in tqdm(Path(opposite_path).rglob('*.pt')):\n",
    "        loaded = torch.load(str(p), map_location=torch.device('cpu'))\n",
    "        opposite_representations.append(loaded)\n",
    "\n",
    "    opposite_representations = torch.cat(opposite_representations, dim=0)\n",
    "    opposite_representations = opposite_representations.permute(1,0,2)\n",
    "\n",
    "    for layer, zipped in enumerate(zip(target_representations, opposite_representations)):\n",
    "        target_representation, opposite_representation = zipped\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(opposite_representation)\n",
    "        target_representation_scaled = scaler.transform(target_representation)\n",
    "        opposite_representation_scaled = scaler.transform(opposite_representation)\n",
    "        pca = PCA(n_components=2).fit(opposite_representation_scaled)\n",
    "        opposite_representation_pca = pca.transform(opposite_representation_scaled)\n",
    "        target_representation_pca = pca.transform(target_representation_scaled)\n",
    "\n",
    "        plt.scatter(target_representation_pca[:,0], target_representation_pca[:,1], color = 'red', alpha = 0.3, s=1, label = 'ditto')\n",
    "        plt.scatter(opposite_representation_pca[:,0], opposite_representation_pca[:,1], color = 'green', alpha = 0.3, s=1, label = 'oppposite')\n",
    "\n",
    "        plt.xlabel('component 0')\n",
    "        plt.ylabel('component 1')\n",
    "        plt.title(f\"Layer {layer}\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = \"/home/sake/MusicGenRepEng_2.5s_ditto_hiddens_batch128_meanssss\"\n",
    "opposite_path = \"/home/sake/MusicGenRepEng_125tokens_uncond_hiddens_batch128_meanssss\"\n",
    "plot_pca(target_path, opposite_path)\n",
    "# plot_pca_over_opposite(target_path, opposite_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = \"/home/sake/MusicGenRepEng_2.5s_ditto_hiddens_batch128_meanssss\"\n",
    "opposite_path = \"/home/sake/MusicGenRepEng_125tokens_uncond_hiddens_batch128_meanssss\"\n",
    "# plot_pca(target_path, opposite_path)\n",
    "plot_pca_over_opposite(target_path, opposite_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MISCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "music, sr = torchaudio.load('/home/sake/MusicGenRepEng_Dataset/Rock/Alternative Rock/Nirvana - Smells Like Teen Spirit.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(music[:,:50].repeat(2,1,1) == torch.stack([music[:,:50], music[:,:50]], dim=0)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=250,\n",
    "    duration=8,\n",
    "    two_step_cfg=False\n",
    ")\n",
    "\n",
    "# Here we use a synthetic signal to prompt both the tonality and the BPM\n",
    "# of the generated audio.\n",
    "res = model.generate_continuation(\n",
    "    music[:,:int(sr*0.02)].repeat(2,1,1),\n",
    "    sr, ['rock, energetic', \n",
    "            'rock, sleepy'], \n",
    "    progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Text Condition Representations(Hidden States)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "music, sr = torchaudio.load('/home/sake/MusicGenRepEng_Dataset/Rock/Alternative Rock/Nirvana - Smells Like Teen Spirit.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=250,\n",
    "    duration=0.02,\n",
    "    two_step_cfg=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = model.get_hidden_states(music[:,:int(sr*0.02)].repeat(2,1,1), sr, [\"fast tempo\", \"slow tempo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes, _ = model._prepare_tokens_and_attributes([\"techno, fast beats, happy, hard, joyful, tribal\"], None)\n",
    "attributes, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.lm.condition_provider(model.lm.condition_provider.tokenize(attributes))\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings['description'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = model.get_hidden_states_text_condition([\"techno, fast beats\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm.condition_provider.conditioners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm.condition_provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm.condition_provider(model.lm.condition_provider.tokenize(attributes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm.condition_provider.conditioners.description(attributes[0]['text']['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm.condition_provider(attributes[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Representations(Hidden States)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "music, sr = torchaudio.load('/home/sake/MusicGenRepEng_Dataset/Rock/Alternative Rock/Nirvana - Smells Like Teen Spirit.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_music = music[:, 20*i*sr:20*(i+1)*sr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = model.get_hidden_states(\n",
    "    input_music, \n",
    "    sr, None, \n",
    "    progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = []\n",
    "for i in range(10):\n",
    "    input_music = music[:, 20*i*sr:20*(i+1)*sr]\n",
    "    rep = model.get_hidden_states(\n",
    "        input_music,\n",
    "        sr, None,\n",
    "        progress=True)\n",
    "    reps.append(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_vec = torch.stack(rep, dim=1)[:,500:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(reps, \"/home/sake/Nirvana - Smells Like Teen Spirit_MusicGenRepEng_Dataset_hidden_states_every20s_10t.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm \n",
    "\n",
    "for path in tqdm(Path('/home/sake/MusicGenRepEng_Dataset_separated').rglob('*.mp3')):\n",
    "    print(\"Representing: \", path)\n",
    "    out_path = str(path).replace('MusicGenRepEng_Dataset', 'MusicGenRepEng_Dataset_hidden_states_30-60_mid10')\n",
    "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    music, sr = torchaudio.load(str(path))\n",
    "    input_music = music[:, 30*sr:50*sr]\n",
    "    rep = model.read_representations(\n",
    "        input_music, \n",
    "        sr, None, \n",
    "        progress=True)\n",
    "    rep_vec = torch.stack(rep, dim=1)[:,500:1000].mean(1)\n",
    "    torch.save(rep_vec, path.with_suffix('.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Control Vector - Ditto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def project_onto_direction(H, direction):\n",
    "    \"\"\"Project matrix H (n, d_1) onto direction vector (d_2,)\"\"\"\n",
    "    mag = np.linalg.norm(direction)\n",
    "    assert not np.isinf(mag)\n",
    "    return (H @ direction) / mag\n",
    "\n",
    "def get_directions(target_path, opposite_path):\n",
    "    target_representations = []\n",
    "    for p in tqdm(natsorted(Path(target_path).rglob('*.pt'), key=str)):\n",
    "        loaded = torch.load(str(p), map_location=torch.device('cpu'))\n",
    "        target_representations.append(loaded)\n",
    "\n",
    "    target_representations = torch.cat(target_representations, dim=0)\n",
    "\n",
    "    opposite_representations = []\n",
    "    for p in tqdm(natsorted(Path(opposite_path).rglob('*.pt'), key=str)):\n",
    "        loaded = torch.load(str(p), map_location=torch.device('cpu'))\n",
    "        opposite_representations.append(loaded)\n",
    "\n",
    "    opposite_representations = torch.cat(opposite_representations, dim=0)\n",
    "\n",
    "    representation_pairs = torch.empty((target_representations.shape[0] * 2, target_representations.shape[1], target_representations.shape[2]))\n",
    "    representation_pairs[0::2] = target_representations\n",
    "    representation_pairs[1::2] = opposite_representations\n",
    "    representation_pairs = representation_pairs.permute(1,0,2)\n",
    "\n",
    "    relative_layer_hiddens = {}\n",
    "    for layer, pair in enumerate(representation_pairs):\n",
    "        relative_layer_hiddens[layer] = (\n",
    "            pair[::2] - pair[1::2]\n",
    "        )\n",
    "\n",
    "    directions = {}\n",
    "    for layer in range(len(relative_layer_hiddens)):\n",
    "        # assert representation_pairs[layer].shape[0] == 110 * 2\n",
    "\n",
    "        # fit layer directions\n",
    "        train = np.vstack(\n",
    "            relative_layer_hiddens[layer].to(\"cpu\").numpy()\n",
    "            - relative_layer_hiddens[layer].to(\"cpu\").numpy().mean(axis=0, keepdims=True)\n",
    "        )\n",
    "        pca_model = PCA(n_components=1, whiten=False).fit(train)\n",
    "        # shape (n_features,)\n",
    "        directions[layer] = pca_model.components_.astype(np.float32).squeeze(axis=0)\n",
    "\n",
    "        # calculate sign\n",
    "        projected_hiddens = project_onto_direction(\n",
    "            representation_pairs[layer].to(\"cpu\").numpy(), directions[layer]\n",
    "        )\n",
    "\n",
    "        # order is [positive, negative, positive, negative, ...]\n",
    "        positive_smaller_mean = np.mean(\n",
    "            [\n",
    "                projected_hiddens[i] < projected_hiddens[i + 1]\n",
    "                for i in range(0, representation_pairs.shape[1], 2)\n",
    "            ]\n",
    "        )\n",
    "        positive_larger_mean = np.mean(\n",
    "            [\n",
    "                projected_hiddens[i] > projected_hiddens[i + 1]\n",
    "                for i in range(0, representation_pairs.shape[1], 2)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if positive_smaller_mean > positive_larger_mean:  # type: ignore\n",
    "            directions[layer] *= -1\n",
    "\n",
    "    return directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_path = \"/home/sake/MusicGenRepEng_2.5s_ditto_hiddens_batch128_from1.5mean01s\"\n",
    "# opposite_path = \"/home/sake/MusicGenRepEng_125tokens_uncond_hiddens_batch128_from1.5mean01s\"\n",
    "\n",
    "target_path = \"/home/sake/MusicGenRepEng_2.5s_ditto_hiddens_batch128_meanssss\"\n",
    "opposite_path = \"/home/sake/MusicGenRepEng_125tokens_uncond_hiddens_batch128_meanssss\"\n",
    "\n",
    "ditto_directions = get_directions(target_path, opposite_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Control Vector - Text Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_onto_direction(H, direction):\n",
    "    \"\"\"Project matrix H (n, d_1) onto direction vector (d_2,)\"\"\"\n",
    "    mag = np.linalg.norm(direction)\n",
    "    assert not np.isinf(mag)\n",
    "    return (H @ direction) / mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directions(path):\n",
    "    representation_pairs = []\n",
    "    for p in tqdm(Path(path).rglob('*.pt')):\n",
    "        loaded = torch.load(str(p), map_location=torch.device('cpu'))[-1]\n",
    "        representation_pairs.append(loaded)\n",
    "\n",
    "    representation_pairs = torch.cat(representation_pairs, dim=0)\n",
    "    representation_pairs = representation_pairs.permute(1,0,2)\n",
    "\n",
    "    relative_layer_hiddens = {}\n",
    "    for layer, pair in enumerate(representation_pairs):\n",
    "        relative_layer_hiddens[layer] = (\n",
    "            pair[::2] - pair[1::2]\n",
    "        )\n",
    "\n",
    "    directions = {}\n",
    "    for layer in range(len(relative_layer_hiddens)):\n",
    "        # assert representation_pairs[layer].shape[0] == 110 * 2\n",
    "\n",
    "        # fit layer directions\n",
    "        train = np.vstack(\n",
    "            relative_layer_hiddens[layer].to(\"cpu\").numpy()\n",
    "            - relative_layer_hiddens[layer].to(\"cpu\").numpy().mean(axis=0, keepdims=True)\n",
    "        )\n",
    "        pca_model = PCA(n_components=1, whiten=False).fit(train)\n",
    "        # shape (n_features,)\n",
    "        directions[layer] = pca_model.components_.astype(np.float32).squeeze(axis=0)\n",
    "\n",
    "        # calculate sign\n",
    "        projected_hiddens = project_onto_direction(\n",
    "            representation_pairs[layer].to(\"cpu\").numpy(), directions[layer]\n",
    "        )\n",
    "\n",
    "        # order is [positive, negative, positive, negative, ...]\n",
    "        positive_smaller_mean = np.mean(\n",
    "            [\n",
    "                projected_hiddens[i] < projected_hiddens[i + 1]\n",
    "                for i in range(0, representation_pairs.shape[1], 2)\n",
    "            ]\n",
    "        )\n",
    "        positive_larger_mean = np.mean(\n",
    "            [\n",
    "                projected_hiddens[i] > projected_hiddens[i + 1]\n",
    "                for i in range(0, representation_pairs.shape[1], 2)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if positive_smaller_mean > positive_larger_mean:  # type: ignore\n",
    "            directions[layer] *= -1\n",
    "\n",
    "    return directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear_directions = get_directions('/home/sake/MusicGenRepEng_Dataset_50ms_clear_poor_mediummodel_norm_nob4layer')\n",
    "energy_directions_b4 = get_directions('/home/sake/MusicGenRepEng_Dataset_50ms_energy_mediummodel_unnorm_b4layer')\n",
    "# energy_directions_nob4 = get_directions('/home/sake/MusicGenRepEng_Dataset_50ms_energy_mediummodel_unnorm_nob4layer')\n",
    "# happy_directions = get_directions('/home/sake/MusicGenRepEng_Dataset_50ms_happy_scary_mediummodel_norm_nob4layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = torch.load(\"/home/sake/MusicGenRepEng_Dataset_50ms_energy_mediummodel_unnorm_b4layer/electronica/Ambient/Ambient Techno/Orbital - Halcyon And On And On.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded[-1][:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([1,2])\n",
    "b = a.to(torch.float16)\n",
    "c = b.numpy()\n",
    "a.dtype, b.dtype, c.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_directions_b4[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Control Vector- Text Pair [MISC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_pairs = []\n",
    "for path in tqdm(Path('/home/sake/MusicGenRepEng_Dataset_50ms_clear_poor_mediummodel_norm_nob4layer').rglob('*.pt')):\n",
    "    loaded = torch.load(str(path))[-1]\n",
    "    representation_pairs.append(loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_pairs = torch.cat(representation_pairs, dim=0)\n",
    "representation_pairs = representation_pairs.permute(1,0,2)\n",
    "representation_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_layer_hiddens = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer, pair in enumerate(representation_pairs):\n",
    "    relative_layer_hiddens[layer] = (\n",
    "        pair[::2] - pair[1::2]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_layer_hiddens[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(48):\n",
    "#     print(i, (relative_layer_hiddens[i][0]==0).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = {}\n",
    "for layer in range(len(relative_layer_hiddens)):\n",
    "    # assert representation_pairs[layer].shape[0] == 110 * 2\n",
    "\n",
    "    # fit layer directions\n",
    "    train = np.vstack(\n",
    "        relative_layer_hiddens[layer].to(\"cpu\").numpy()\n",
    "        - relative_layer_hiddens[layer].to(\"cpu\").numpy().mean(axis=0, keepdims=True)\n",
    "    )\n",
    "    pca_model = PCA(n_components=1, whiten=False).fit(train)\n",
    "    # shape (n_features,)\n",
    "    directions[layer] = pca_model.components_.astype(np.float32).squeeze(axis=0)\n",
    "\n",
    "    # calculate sign\n",
    "    projected_hiddens = project_onto_direction(\n",
    "        representation_pairs[layer].to(\"cpu\").numpy(), directions[layer]\n",
    "    )\n",
    "\n",
    "    # order is [positive, negative, positive, negative, ...]\n",
    "    positive_smaller_mean = np.mean(\n",
    "        [\n",
    "            projected_hiddens[i] < projected_hiddens[i + 1]\n",
    "            for i in range(0, representation_pairs.shape[1], 2)\n",
    "        ]\n",
    "    )\n",
    "    positive_larger_mean = np.mean(\n",
    "        [\n",
    "            projected_hiddens[i] > projected_hiddens[i + 1]\n",
    "            for i in range(0, representation_pairs.shape[1], 2)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if positive_smaller_mean > positive_larger_mean:  # type: ignore\n",
    "        directions[layer] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions[47].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_directions = directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_directions = directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Control Vector - A song pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ditto_reps = torch.load(\"/home/sake/Ditto-2-NewJeans_MusicGenRepEng_Dataset_hidden_states_every20s_10t.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dexter_reps = torch.load(\"/home/sake/Ricardo Villalobos - Dexter [SED008]_MusicGenRepEng_Dataset_hidden_states_every20s_10t.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ditto_reps[0][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.stack([rep[-1] for rep in ditto_reps], dim=0)\n",
    "target = target.squeeze(1).permute(1, 0, 2).cpu()\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = torch.stack([rep[-1] for rep in dexter_reps], dim=0)\n",
    "reps = reps.squeeze(1).permute(1, 0, 2).cpu()\n",
    "reps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = torch.stack([rep[-1] for rep in reps], dim=0)\n",
    "reps = reps.squeeze(1).permute(1, 0, 2).cpu()\n",
    "reps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_onto_direction(H, direction):\n",
    "    \"\"\"Project matrix H (n, d_1) onto direction vector (d_2,)\"\"\"\n",
    "    mag = np.linalg.norm(direction)\n",
    "    assert not np.isinf(mag)\n",
    "    return (H @ direction) / mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Difference\n",
    "\n",
    "diffs = target.cpu() - reps.cpu() # target - others (pos - neg)\n",
    "diffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg or Last Hidden State\n",
    "\n",
    "directions = {}\n",
    "for layer in tqdm(range(diffs.shape[0])):\n",
    "    # assert diff[layer].shape[0] == len(inputs) * 2\n",
    "\n",
    "    # fit layer directions\n",
    "    train = np.vstack(\n",
    "        diffs[layer]\n",
    "        # - diffs[layer].mean(axis=0, keepdims=True)\n",
    "    )\n",
    "    pca_model = PCA(n_components=1, whiten=False).fit(train)\n",
    "    # shape (n_features,)\n",
    "    directions[layer] = pca_model.components_.astype(np.float32).squeeze(axis=0)\n",
    "    # print(directions[layer].shape)\n",
    "    # calculate sign\n",
    "    # projected_hiddens = project_onto_direction(\n",
    "    #     reps[layer], directions[layer]\n",
    "    # )\n",
    "    # # print(projected_hiddens[0])\n",
    "    # target_projected_hiddens = project_onto_direction(\n",
    "    #     target[layer], directions[layer]\n",
    "    # )\n",
    "    # # print(target_projected_hiddens[0])\n",
    "\n",
    "    # # order is [positive, negative, positive, negative, ...]\n",
    "    # positive_smaller_mean = np.mean(\n",
    "    #     [\n",
    "    #         target_projected_hiddens[0] < projected_hiddens[i] # target is smaller than others\n",
    "    #         for i in range(0, reps.shape[1])\n",
    "    #     ]\n",
    "    # )\n",
    "    # positive_larger_mean = np.mean(\n",
    "    #     [\n",
    "    #         target_projected_hiddens[0] > projected_hiddens[i] # target is larger than others\n",
    "    #         for i in range(0, reps.shape[1])\n",
    "    #     ]\n",
    "    # )\n",
    "\n",
    "    # if positive_smaller_mean > positive_larger_mean:  # type: ignore\n",
    "    #     directions[layer] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ditto_smells_directions = directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Control Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = []\n",
    "for path in tqdm(Path('/home/sake/MusicGenRepEng_Dataset_hidden_states_30-60_mid10_non_avg_smallmodel').rglob('*.pt')):\n",
    "    rep = torch.load(path)\n",
    "    reps.append(rep.cpu())\n",
    "reps = torch.stack(reps)\n",
    "reps = reps.squeeze(1)\n",
    "# reps = reps.permute(1, 0, 2) # (layers, batch, hidden_states)\n",
    "reps = reps.permute(2, 0, 1, 3) # (layers, batch, timesteps, hidden_states)\n",
    "# reps = reps[:,:,-1] # Last hidden_state\n",
    "reps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.load(\"/home/sake/MusicGenRepEng_Dataset_hidden_states_30-60_mid10/Ditto-2-NewJeans.pt\")\n",
    "target = target.permute(1, 0, 2).cpu() # (layers, batch, hidden_states)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = rep_vec.cpu()\n",
    "target = target.permute(2, 0, 1, 3) # (layers, batch, timesteps, hidden_states)\n",
    "# target = target[:,:,-1] # Last hidden_state\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Difference\n",
    "\n",
    "diffs = target.cpu() - reps.cpu() # target - others (pos - neg)\n",
    "diffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_onto_direction(H, direction):\n",
    "    \"\"\"Project matrix H (n, d_1) onto direction vector (d_2,)\"\"\"\n",
    "    mag = np.linalg.norm(direction)\n",
    "    assert not np.isinf(mag)\n",
    "    return (H @ direction) / mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg or Last Hidden State\n",
    "\n",
    "directions = {}\n",
    "for layer in tqdm(range(diffs.shape[0])):\n",
    "    # assert diff[layer].shape[0] == len(inputs) * 2\n",
    "\n",
    "    # fit layer directions\n",
    "    train = np.vstack(\n",
    "        diffs[layer]\n",
    "        - diffs[layer].mean(axis=0, keepdims=True)\n",
    "    )\n",
    "    pca_model = PCA(n_components=1, whiten=False).fit(train)\n",
    "    # shape (n_features,)\n",
    "    directions[layer] = pca_model.components_.astype(np.float32).squeeze(axis=0)\n",
    "    # print(directions[layer].shape)\n",
    "    # calculate sign\n",
    "    projected_hiddens = project_onto_direction(\n",
    "        reps[layer], directions[layer]\n",
    "    )\n",
    "    # print(projected_hiddens[0])\n",
    "    target_projected_hiddens = project_onto_direction(\n",
    "        target[layer], directions[layer]\n",
    "    )\n",
    "    # print(target_projected_hiddens[0])\n",
    "\n",
    "    # order is [positive, negative, positive, negative, ...]\n",
    "    positive_smaller_mean = np.mean(\n",
    "        [\n",
    "            target_projected_hiddens[0] < projected_hiddens[i] # target is smaller than others\n",
    "            for i in range(0, reps.shape[1])\n",
    "        ]\n",
    "    )\n",
    "    positive_larger_mean = np.mean(\n",
    "        [\n",
    "            target_projected_hiddens[0] > projected_hiddens[i] # target is larger than others\n",
    "            for i in range(0, reps.shape[1])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if positive_smaller_mean > positive_larger_mean:  # type: ignore\n",
    "        directions[layer] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_diffs = diffs.flatten(1,2).cpu()\n",
    "f_target = target.flatten(1,2).cpu()\n",
    "f_reps = reps.flatten(1,2).cpu()\n",
    "f_diffs.shape, f_target.shape, f_reps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Hidden States\n",
    "\n",
    "\n",
    "directions = {}\n",
    "for layer in tqdm(range(f_diffs.shape[0])):\n",
    "    # assert diff[layer].shape[0] == len(inputs) * 2\n",
    "\n",
    "    # fit layer directions\n",
    "    train = np.vstack(\n",
    "        f_diffs[layer]\n",
    "        - f_diffs[layer].mean(axis=0, keepdims=True)\n",
    "    )\n",
    "    pca_model = PCA(n_components=1, whiten=False).fit(train)\n",
    "    # shape (n_features,)\n",
    "    directions[layer] = pca_model.components_.astype(np.float32).squeeze(axis=0)\n",
    "    # calculate sign\n",
    "    projected_hiddens = project_onto_direction(\n",
    "        f_reps[layer], directions[layer]\n",
    "    )\n",
    "    target_projected_hiddens = project_onto_direction(\n",
    "        f_target[layer], directions[layer]\n",
    "    )\n",
    "\n",
    "    # order is [positive, negative, positive, negative, ...]\n",
    "    positive_smaller_mean = np.mean(\n",
    "        [\n",
    "            target_projected_hiddens[i%f_target.shape[1]] < projected_hiddens[i] # target is smaller than others\n",
    "            for i in range(0, f_reps.shape[1])\n",
    "        ]\n",
    "    )\n",
    "    positive_larger_mean = np.mean(\n",
    "        [\n",
    "            target_projected_hiddens[i%f_target.shape[1]] > projected_hiddens[i] # target is larger than others\n",
    "            for i in range(0, f_reps.shape[1])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if positive_smaller_mean > positive_larger_mean:  # type: ignore\n",
    "        directions[layer] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(directions, \"/home/sake/Ditto-2-NewJeans_MusicGenRepEng_Dataset_hidden_states_30-60_non_avg_smallmodel_directions.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with Control Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.utils.notebook import display_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "music, sr = torchaudio.load('/home/sake/ditto_vocalless.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "# From https://gist.github.com/gatheluck/c57e2a40e3122028ceaecc3cb0d152ac\n",
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=250,\n",
    "    duration=20,\n",
    "    two_step_cfg=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(41)\n",
    "res = model.generate_continuation(\n",
    "    music[:,int(sr*30.00):int(sr*30.1)].repeat(n,1,1), sr, ['']*n, \n",
    "    progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,int(sr*30.00):int(sr*30.10)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[ditto_directions, ditto_dist_mean], coefficients=[1], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 8\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"\"]*n, control_vectors=[ditto_directions, ditto_dist_mean], coefficients=[+1], sustains=[0], ramps=[50],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"\"]*n, control_vectors=[ditto_directions, ditto_dist_mean], coefficients=[1], sustains=[1000], ramps=[10],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_with_control_vectors(descriptions=[\"\"]*n, control_vectors=[ditto_directions, ditto_dist_mean], coefficients=[2], sustains=[0], ramps=[10],\n",
    "                                                       before_layer=False, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation(\n",
    "    music[:,:int(sr*5.00)].repeat(n,1,1), sr, ['rock, extremely sleepy']*n, \n",
    "    progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[0.3], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"pop\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.3], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"pop\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[0.1], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"pop\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[0.2], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"pop\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.1], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"pop\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.2], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"pop\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[0], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"pop\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[0.15], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[None]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[0.2], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[None]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[0], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[None]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.15], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[None]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.2], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[None]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cocoeff 25 + clear\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions, clear_directions], coefficients=[0.1, 0.1], sustains=[1000]*2, ramps=[250]*2,\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cocoeff 25 + clear\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions, clear_directions], coefficients=[-0.1, 0.1], sustains=[1000]*2, ramps=[250]*2,\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cocoeff 25 + clear\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions, clear_directions], coefficients=[0.1, -0.1], sustains=[1000]*2, ramps=[250]*2,\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cocoeff 25 + clear\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions, clear_directions], coefficients=[-0.1, -0.1], sustains=[1000]*2, ramps=[250]*2,\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cocoeff 25 cvjazz\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions_jazz], coefficients=[0.2], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cocoeff 10 cvjazz\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions_jazz], coefficients=[0.1], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cocoeff 10 cvjazz\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions_jazz], coefficients=[-0.1], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cocoeff 100 cvjazz\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions_jazz], coefficients=[0.1], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cocoeff 100 cvjazz\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions_jazz], coefficients=[-0.1], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cocoeff 50 cvjazz\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions_jazz], coefficients=[0.1], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cocoeff 50 cvjazz\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions_jazz], coefficients=[-0.1], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cocoeff 25 cvjazz\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions_jazz], coefficients=[-0.2], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cocoeff 25\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[0.2], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cocoeff 25\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.2], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cocoeff 25\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[0.1], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cocoeff 25\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.1], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cocoeff 50\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[0.1], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cocoeff 50\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.1], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cocoeff 50\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.05], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rate 50\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[0.1], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rate 50\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.1], sustains=[1000], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ 12\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.1], sustains=[500], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ 24\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[0.05], sustains=[500], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ 24\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.05], sustains=[500], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24~\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[0.05], sustains=[500], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24~\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.05], sustains=[500], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# > 36\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[0.05], sustains=[500], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# > 36\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.05], sustains=[500], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 48 stairs\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[0.1], sustains=[500], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 48 stairs\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.1], sustains=[500], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"jazz\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy +0.05 energy +0.05\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.05], sustains=[500], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"rock, sleepy\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy +0.05 energy +0.05\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.1], sustains=[500], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"rock, sleepy\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy +0.05 energy +0.05\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.1], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"rock, sleepy\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy +0.05 energy +0.05\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.2], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"rock, sleepy\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy +0.05 energy +0.05\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.05], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"rock\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy +0.05 energy +0.05\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.05], sustains=[500], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"rock\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy +0.05 energy +0.05\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.2], sustains=[500], ramps=[500],\n",
    "                                                       before_layer=False, descriptions=[\"rock\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy +0.05 energy +0.05\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.1], sustains=[750], ramps=[500],\n",
    "                                                       before_layer=False, descriptions=[\"rock\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy +0.05 energy +0.05\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.15], sustains=[500], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"rock\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy +0.05 energy +0.05\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.12], sustains=[500], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"rock\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy +0.05 energy +0.05\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.07], sustains=[500], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"rock\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy +0.05 energy +0.05\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.2], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"rock\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy +0.05 energy +0.05\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.3], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"rock\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy +0.05 energy +0.05\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.3], sustains=[50], ramps=[150],\n",
    "                                                       before_layer=False, descriptions=[\"rock\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy +0.05 energy +0.05\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.5], sustains=[1], ramps=[10],\n",
    "                                                       before_layer=False, descriptions=[\"rock\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy +0.05 energy +0.05\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5.00)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[energy_directions], coefficients=[-0.05], sustains=[100], ramps=[250],\n",
    "                                                       before_layer=False, descriptions=[\"pop\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy +0.05 energy +0.05\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*0.02)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[happy_directions, energy_directions], coefficients=[0.05, 0.5], sustains=[100, 100], ramps=[250, 250],\n",
    "                                                       before_layer=False, descriptions=[\"pop\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy +0.05 energy -0.05\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*0.02)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[happy_directions, energy_directions], coefficients=[0.05, -0.05], sustains=[100, 100], ramps=[250, 250],\n",
    "                                                       before_layer=False, descriptions=[\"pop\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy -0.05 energy +0.05\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*0.02)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[happy_directions, energy_directions], coefficients=[-0.05, +0.05], sustains=[100, 100], ramps=[250, 250],\n",
    "                                                       before_layer=False, descriptions=[\"pop\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy -0.05 energy -0.05\n",
    "n = 4\n",
    "set_all_seeds(42)\n",
    "res = model.generate_continuation_with_control_vectors(music[:,:int(sr*0.02)].repeat(n,1,1), sr, \n",
    "                                                       control_vectors=[happy_directions, energy_directions], coefficients=[-0.05, -0.05], sustains=[100, 100], ramps=[250, 250],\n",
    "                                                       before_layer=False, descriptions=[\"pop\"]*n, progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(42)\n",
    "output = model.generate(\n",
    "    descriptions=[\n",
    "        'happy jazz'\n",
    "    ]*4 + ['jazz, happy']*4,\n",
    "    progress=True, return_tokens=True\n",
    ")\n",
    "display_audio(output[0], sample_rate=32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(42)\n",
    "output = model.generate(\n",
    "    descriptions=[\n",
    "        'sad jazz'\n",
    "    ]*4 + ['jazz, sad']*4,\n",
    "    progress=True, return_tokens=True\n",
    ")\n",
    "display_audio(output[0], sample_rate=32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(42)\n",
    "output = model.generate(\n",
    "    descriptions=[\n",
    "        'rock, clear sound'\n",
    "    ]*4 + ['rock, poor sound']*4,\n",
    "    progress=True, return_tokens=True\n",
    ")\n",
    "display_audio(output[0], sample_rate=32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(42)\n",
    "output = model.generate(\n",
    "    descriptions=[\n",
    "        'rock, high fidelity'\n",
    "    ]*4 ,\n",
    "    progress=True, return_tokens=True\n",
    ")\n",
    "display_audio(output[0], sample_rate=32000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torchaudio\n",
    "import torch\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "def get_bip_bip(bip_duration=0.125, frequency=440,\n",
    "                duration=0.5, sample_rate=32000, device=\"cuda\"):\n",
    "    \"\"\"Generates a series of bip bip at the given frequency.\"\"\"\n",
    "    t = torch.arange(\n",
    "        int(duration * sample_rate), device=\"cuda\", dtype=torch.float) / sample_rate\n",
    "    wav = torch.cos(2 * math.pi * 440 * t)[None]\n",
    "    tp = (t % (2 * bip_duration)) / (2 * bip_duration)\n",
    "    envelope = (tp >= 0.5).float()\n",
    "    return wav * envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use a synthetic signal to prompt both the tonality and the BPM\n",
    "# of the generated audio.\n",
    "res = model.generate_continuation(\n",
    "    get_bip_bip(0.125).expand(2, -1, -1), \n",
    "    32000, ['Jazz jazz and only jazz', \n",
    "            'Heartful EDM with beautiful synths and chords'], \n",
    "    progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use any audio from a file. Make sure to trim the file if it is too long!\n",
    "prompt_waveform, prompt_sr = torchaudio.load(\"../assets/bach.mp3\")\n",
    "prompt_duration = 2\n",
    "prompt_waveform = prompt_waveform[..., :int(prompt_duration * prompt_sr)]\n",
    "output = model.generate_continuation(prompt_waveform, prompt_sample_rate=prompt_sr, progress=True, return_tokens=True)\n",
    "display_audio(output[0], sample_rate=32000)\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    out_diffusion = mbd.tokens_to_wav(output[1])\n",
    "    display_audio(out_diffusion, sample_rate=32000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-conditional Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "output = model.generate(\n",
    "    descriptions=[\n",
    "        #'80s pop track with bassy drums and synth',\n",
    "        #'90s rock song with loud guitars and heavy drums',\n",
    "        #'Progressive rock drum and bass solo',\n",
    "        #'Punk Rock song with loud drum and power guitar',\n",
    "        #'Bluesy guitar instrumental with soulful licks and a driving rhythm section',\n",
    "        #'Jazz Funk song with slap bass and powerful saxophone',\n",
    "        'drum and bass beat with intense percussions'\n",
    "    ],\n",
    "    progress=True, return_tokens=True\n",
    ")\n",
    "display_audio(output[0], sample_rate=32000)\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    out_diffusion = mbd.tokens_to_wav(output[1])\n",
    "    display_audio(out_diffusion, sample_rate=32000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melody-conditional Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "model = MusicGen.get_pretrained('facebook/musicgen-melody')\n",
    "model.set_generation_params(duration=8)\n",
    "\n",
    "melody_waveform, sr = torchaudio.load(\"../assets/bach.mp3\")\n",
    "melody_waveform = melody_waveform.unsqueeze(0).repeat(2, 1, 1)\n",
    "output = model.generate_with_chroma(\n",
    "    descriptions=[\n",
    "        '80s pop track with bassy drums and synth',\n",
    "        '90s rock song with loud guitars and heavy drums',\n",
    "    ],\n",
    "    melody_wavs=melody_waveform,\n",
    "    melody_sample_rate=sr,\n",
    "    progress=True, return_tokens=True\n",
    ")\n",
    "display_audio(output[0], sample_rate=32000)\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    out_diffusion = mbd.tokens_to_wav(output[1])\n",
    "    display_audio(out_diffusion, sample_rate=32000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_name = \"LARGE_ENERGY_AFTERLAYER_NORMED_EVERYLAYER\"\n",
    "n = 8\n",
    "sustain = 1500\n",
    "ramp = 0\n",
    "for path in tqdm(Path('/home/sake/demo songs').rglob('*.mp3')):\n",
    "    genres = path.name.split('-')[0].split('_').append(None)\n",
    "    for genre in genres:\n",
    "        for coeff in range(-6, 7, 1):\n",
    "            set_all_seeds(42)\n",
    "            res = model.generate_continuation_with_control_vectors(music[:,:int(sr*5)].repeat(n,1,1), sr, \n",
    "                                                                control_vectors=[energy_directions], coefficients=[coeff/20.0], sustains=[sustain], ramps=[ramp],\n",
    "                                                                before_layer=False, descriptions=[genre]*n, progress=True)\n",
    "            for out in res:\n",
    "                if genre is None:\n",
    "                    genre_t = \"None\"\n",
    "                else:\n",
    "                    genre_t = genre\n",
    "                out_path = f\"/home/sake/grid_inference_outputs/{grid_name}/sus_{sustain}_ramp_{ramp}/{path.name.split('-')[1]}/{genre_t}/{coeff}.mp3\"\n",
    "                Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "                torchaudio.save(out_path, out, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in tqdm(Path('/home/sake/demo songs').rglob('*.mp3')):\n",
    "    print(path.name.split('-')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coeff in range(-3, 4, 1):\n",
    "    print(coeff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b02c911f9b3627d505ea4a19966a915ef21f28afb50dbf6b2115072d27c69103"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
